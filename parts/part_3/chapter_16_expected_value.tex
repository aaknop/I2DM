\chapter{Expectation of a Random Variable}
\label{chapter:expectation}
To understand the behaviour of a random variable one may try to use the average
value; there are several ways to define the average value, but we are going to
concentrate on the definition based on the mean. Given a random variable $X$
in the finite discrete probability space $(\Omega, \Distribution{D})$, the
expected value $\expectation[\Distribution{D}]{X}$ 
(or $\expectation[\omega \gets \Distribution{D}]{X(\omega)}$) of $\chi$ is equal to
$\sum_{\omega \in \Omega} \Pr_{\Distribution{D}}(\omega) X(\omega)$.

Note that it is possible that there are several $\omega$'s with the same
$X(\omega)$. Hence, one may give an alternative definition of the expectation.
\begin{theorem}
  Let $(\Omega, \Distribution{D})$ be a finite discrete probability space, and
  let $X$ be a random variable in the probability space. Then
  $\expectation[\Distribution{D}]{X} =
  \sum_{a \in \Im X} a\Pr_{\Distribution{D}}(X = a)$.
\end{theorem}

An important property of the expectation that often allows simplifications in
computing the expected value of a random variable is its linearity.
\begin{theorem}
  Let $(\Omega, \Distribution{D})$ be a finite discrete probability space; $X$
  and $Y$ be random variables; and let $\lambda \in \R$.
  Then $\expectation[\Distribution{D}]{X + Y} = 
    \expectation[\Distribution{D}]{X} + \expectation[\Distribution{D}]{Y}$ and
  $\expectation[\Distribution{D}]{\lambda X} = \lambda
  \expectation[\Distribution{D}]{X}$.
\end{theorem}
Let us give a simple example showing that this theorem can help to compute the
expected value of a random variable. Consider an experiment consisting of
tossing $n$ standard coins; i.e. consider the finite discrete probability space
$(\Omega, \Pr)$ such that $\Omega = \set{H, T}^n$ and $\Uniform{\Omega}$ is the
uniform distribution on $\Omega$. We would like to find the expected number $X$
of heads in the experiment. Let $X_i$ be the random variable that is equal to
$1$ if the $i$th flip yields heads, otherwise it is equal to $0$. It is clear
that $X = \sum_{i = 1}^n X_i$. Hence, $\expectation[\Uniform{\Omega}]{X} = 
\sum_{i = 1}^n \expectation[\Uniform{\Omega}]{X_i}$.
However, $\expectation[\Uniform{\Omega}]{X_i} = 1 / 2$ which implies that
$\expectation[\Uniform{\Omega}]{X} = n / 2$.

Let us use this knowledge to study a game similar to the game discussed in
\Cref{chapter:structural-induciton}. Alice selected $500$ numbers from $1$ to
$1000$ and Bob would like to guess at least one of them. How many questions Bob
need to ask to do this?

Apparently, the situation is drastically different if Bob's algorithm can be
randomized and if it cannot be randomized. To show this we need to extend the
definition of $B$-decision trees so that they can operate not only with
integers.
\begin{definition}
  Let $X$ and $Y$ be some sets. We say that $T$ is a $B$-decision tree if
  \begin{description}
    \item [(base case)] either $T$ is equal to $\DTReturn{y}$ for $y \in Y$, or
    \item [(recursion step)] $T$ is equal to $\DTIf{f}{T_0}{T_1}$,
      where $f : X \to \set{0, 1}$, and $T_0$ and $T_1$
      are $B$-decision trees.
  \end{description}

  The number of queries $\DTHeight[x]{T}$ of $T$ at $x \in X$ can be defined as
  follows.
  \begin{description}
    \item [(base case)] Let $T$ be equal to $\DTReturn{y}$, where$y \in Y$. Then
      $\DTHeight[x]{T} = 0$ for all $x \in X$.
    \item[(recursion step)] Let $T$ be equal to $\DTIf{f}{T_0}{T_1}$. Then
      \[
          \DTHeight[x]{T} = 
          \begin{cases}
            \DTHeight[x]{T_0} + 1 & \text{if } f(x) = 0 \\
            \DTHeight[x]{T_1} + 1 & \text{otherwise}
          \end{cases}
      \]
  \end{description}

  The value $\DTValue{T}{x}$ of a $B$-decision tree $T$ at $x \in X$
  can be defined as follows.
  \begin{description}
      \item [(base case)] Let $T$ be equal to $\DTReturn{y}$, where $y \in Y$.
        Then $\DTValue{T}{x} = y$.
      \item[(recursion step)] Let $T$ be equal to $\DTIf{f}{T_0}{T_1}$. Then
        \[
          \DTValue{T}{x} =
          \begin{cases} 
            \DTValue{T_0}{x} & \text{if } f(x) = 0 \\
            \DTValue{T_1}{x} & \text{otherwise}
          \end{cases}.
        \]
  \end{description}
\end{definition}

\begin{theorem}
\label{theorem:guess-one-out-of-many}
  Let $\binom{\range{1000}}{500}$ denote the set of subsets of $\range{1000}$
  with $500$ elements.\footnote{%
    We are going to discuss such sets in \Cref{chapter:binomials}.
  }
  \begin{itemize}
    \item Let $T$ be a $B$-decision tree such that $\DTValue{T}{S} \in S$
      for all $S \in \binom{\range{1000}}{500}$. Then $\DTHeight{T} \ge 9$.
    \item There are a set $\Omega$ of $B$-decision trees and a probability
      distribution $\Distribution{D}$ on $\Omega$ such that 
      $\DTValue{T}{S} \in S$ for all $S \in \binom{\range{1000}}{500}$ and 
      $T \in \Omega$, but 
      $\expectation[\Distribution{D}]{\DTValue{T}{S}} \le 2$ for all 
      $S \in \binom{\range{1000}}{500}$.
  \end{itemize}
\end{theorem}
\begin{proof}
  We prove only the second part of the statement, the first part can be proven
  similarly to \Cref{theorem:guess-the-number}.
  To prove this statement let us consider the following algorithm.
  \begin{itemize}
    \item Choose $x_1, \dots, x_n \in [1000]$ uniformly at random;
    \item Set $i = 1$;
    \item Query whether $x_i \in S$ or not.
    \item If yes, the output is $x_i$, otherwise increase $i$.
    \item If $i \le n$ go to step $3$, otherwise go to step $6$.
    \item Bruteforce all the numbers from $[1000]$ and check whether they belong
      to $S$ or not.
  \end{itemize}
  Let us compute the expected number of queries made by this algorithm. It is
  clear that the probability that the algorithm gets yes for $i = k$ is equal to
  $\left(\frac{1}{2}\right)^k$. Hence, the expected number of queries is equal
  to 
  \[
    \left(\frac{1}{2}\right)^n 1000 + 
    \sum_{k = 1}^n \left(\frac{1}{2}\right)^k k.
  \]

  The following claim gives the formula that allows to compute this
  sum.\footnote{%
    We are going to discuss a method to guess such formulas in
    \Cref{chapter:binomials}.
  }
  \begin{claim}
  \label{claim:guess-one-out-of-many}
      For any $k \in \N$, $\sum_{k = 1}^n \left(\frac{1}{2}\right)^k k = 
      2 - \frac{n + 2}{2^n}$.
  \end{claim}

  Therefore, the average number of queries is at most 
  $2 - \frac{n + 998}{2^n} \le 2$ for $n \ge 10$.

  It is also clear that the result of this algorithm is always correct.

  To finish the proof we need to prove \Cref{claim:guess-one-out-of-many}.
  We prove it using induction by $n$. It is clear that the statement is true for
  $n = 0$. Assume the statement is true for $n$; i.e., 
  $\sum_{k = 1}^n \left(\frac{1}{2}\right)^k k = 2^{-n} (-n + 2^{n + 1} - 2)$.
  This implies that 
  \begin{multline*}
    \sum_{k = 1}^{n + 1} \left(\frac{1}{2}\right)^k k = 
    2 - \frac{n + 2}{2^n} + \frac{n + 1}{2^{n + 1}} = \\
    2 - \frac{2n + 4 - n - 1}{2^{n + 1}} = 2 - \frac{(n + 1) + 2}{2^{n + 1}}.
  \end{multline*}
  In other words, the statement is true for $n + 1$. Hence, by the induction
  principle, we proved the statement for all $n$.
\end{proof}

\begin{chapterendexercises}
  \exercise Prove the first part of \Cref{theorem:guess-one-out-of-many}.
\end{chapterendexercises}
